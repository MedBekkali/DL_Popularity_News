{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def latest_file(pattern: str):\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        return None\n",
    "    return sorted(files, key=os.path.getmtime)[-1]\n",
    "\n",
    "def show_with_explanation(title: str, explanation: str, save_path: str = None):\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(explanation)\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "def safe_bar_labels(ax):\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        ax.annotate(f\"{h:.3f}\", (p.get_x() + p.get_width()/2, h), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "\n",
    "RESULTS_JSON = latest_file(\"results_torch_*.json\")\n",
    "PREDS_NPZ    = latest_file(\"preds_torch_*.npz\")\n",
    "\n",
    "if RESULTS_JSON is None:\n",
    "    raise FileNotFoundError(\"No results_torch_*.json found. Run the training cell first.\")\n",
    "if PREDS_NPZ is None:\n",
    "    raise FileNotFoundError(\"No preds_torch_*.npz found. Run training with: python -m src.train_torch --save_pred\")\n",
    "\n",
    "with open(RESULTS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    R = json.load(f)\n",
    "\n",
    "P = np.load(PREDS_NPZ)\n",
    "\n",
    "y_test = P[\"y_test\"].astype(int)\n",
    "proba  = P[\"proba_test\"].astype(float)\n",
    "pred   = P[\"pred_test\"].astype(int)\n",
    "\n",
    "Y_test = P[\"Y_test\"].astype(float)      # (n,3)\n",
    "Y_pred = P[\"Y_pred_test\"].astype(float) # (n,3)\n",
    "\n",
    "cls = R[\"results\"][\"classification\"]\n",
    "reg = R[\"results\"][\"regression\"]\n",
    "\n",
    "print(\"Loaded:\", RESULTS_JSON, \"and\", PREDS_NPZ)\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = \"figures\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cls[\"train_history\"][\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(cls[\"train_history\"][\"val_loss\"],   label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — Learning Curves (Train vs Validation Loss)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Diagnose optimization and overfitting/underfitting for the classifier.\\n\"\n",
    "        \"How to read: If train loss keeps decreasing while validation loss stops improving or increases, \"\n",
    "        \"the model is overfitting. If both plateau early at high values, the model may be underfitting.\\n\"\n",
    "        \"Idea: Early stopping typically triggers when validation stops improving, preventing overfit.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"01_classification_learning_curves.png\")\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reg[\"train_history\"][\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(reg[\"train_history\"][\"val_loss\"],   label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE on standardized targets)\")\n",
    "plt.legend()\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Regression — Learning Curves (Train vs Validation Loss)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Diagnose optimization and generalization for the multi-output regressor.\\n\"\n",
    "        \"How to read: Loss here is MSE computed on standardized targets, so it is comparable across outputs.\\n\"\n",
    "        \"Idea: A stable gap (train < val) is normal. A growing gap indicates overfitting; \"\n",
    "        \"both flat high indicates underfitting or insufficient features.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"02_regression_learning_curves.png\")\n",
    ")\n",
    "\n",
    "cm = np.array(cls[\"mlp\"][\"confusion_matrix\"])\n",
    "fig, ax = plt.subplots()\n",
    "ConfusionMatrixDisplay(cm, display_labels=[\"Non-viral (0)\", \"Viral (1)\"]).plot(ax=ax, values_format=\"d\")\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — Confusion Matrix (Test Set)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Show the types of classification errors.\\n\"\n",
    "        \"How to read: Rows = true class, Columns = predicted class.\\n\"\n",
    "        \" - Top-left (TN): correctly predicted non-viral.\\n\"\n",
    "        \" - Bottom-right (TP): correctly predicted viral.\\n\"\n",
    "        \" - Top-right (FP): predicted viral but actually non-viral.\\n\"\n",
    "        \" - Bottom-left (FN): predicted non-viral but actually viral.\\n\"\n",
    "        \"Idea: For imbalanced data, this is more informative than accuracy alone.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"03_confusion_matrix.png\")\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(proba, bins=30)\n",
    "plt.xlabel(\"Predicted probability of 'viral'\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — Distribution of Predicted Probabilities (Test)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Check model confidence and calibration behavior.\\n\"\n",
    "        \"How to read: A strong model often separates probabilities toward 0 and 1. \"\n",
    "        \"If most values cluster near 0.5, the classifier is uncertain.\\n\"\n",
    "        \"Idea: This helps justify threshold tuning (choosing a decision cutoff different from 0.5).\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"04_probability_histogram.png\")\n",
    ")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random baseline\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — ROC Curve (Test)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Evaluate ranking quality across all thresholds.\\n\"\n",
    "        \"How to read: The curve shows TPR vs FPR for every possible threshold. \"\n",
    "        \"AUC summarizes the curve: 0.5 = random, 1.0 = perfect ranking.\\n\"\n",
    "        \"Idea: ROC is useful, but with class imbalance, Precision–Recall can be more informative.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"05_ROC_curve.png\")\n",
    ")\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba)\n",
    "ap = average_precision_score(y_test, proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f\"Average Precision (AP) = {ap:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — Precision–Recall Curve (Test)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Evaluate performance on the positive (viral) class under imbalance.\\n\"\n",
    "        \"How to read: Precision measures how many predicted virals are correct; Recall measures \"\n",
    "        \"how many true virals are found.\\n\"\n",
    "        \"Idea: PR curves are often preferred for imbalanced datasets because they focus on the positive class.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"06_PR_curve.png\")\n",
    ")\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 19)\n",
    "f1s = [f1_score(y_test, (proba >= thr).astype(int), average=\"macro\") for thr in thresholds]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, f1s, marker=\"o\")\n",
    "plt.xlabel(\"Decision threshold\")\n",
    "plt.ylabel(\"Macro F1\")\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Classification — Macro-F1 as a Function of the Threshold (Test)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Visualize why the chosen threshold may not be 0.5.\\n\"\n",
    "        \"How to read: The peak indicates the threshold that best balances classes under macro-F1.\\n\"\n",
    "        \"Idea: This supports your project decision to tune the threshold on validation (and then apply it to test).\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"07_F1_vs_threshold.png\")\n",
    ")\n",
    "\n",
    "target_names = [\"y1 (log1p(shares))\", \"y2 (log1p(capped shares))\", \"y3 (percentile score)\"]\n",
    "\n",
    "for j, name in enumerate(target_names):\n",
    "    yt = Y_test[:, j]\n",
    "    yp = Y_pred[:, j]\n",
    "\n",
    "    # True vs Pred\n",
    "    plt.figure()\n",
    "    plt.scatter(yt, yp, s=10)\n",
    "    mn = min(yt.min(), yp.min())\n",
    "    mx = max(yt.max(), yp.max())\n",
    "    plt.plot([mn, mx], [mn, mx], linestyle=\"--\", label=\"Ideal: y = x\")\n",
    "    plt.xlabel(\"True value\")\n",
    "    plt.ylabel(\"Predicted value\")\n",
    "    plt.legend()\n",
    "    show_with_explanation(\n",
    "        title=f\"Regression — {name} — True vs Predicted (Test)\",\n",
    "        explanation=(\n",
    "            \"Purpose: Check how close predictions are to ground truth.\\n\"\n",
    "            \"How to read: Points close to the diagonal mean good predictions. Systematic deviations \"\n",
    "            \"indicate bias (e.g., underpredicting high values).\\n\"\n",
    "            \"Idea: For heavy-tailed original shares, using log targets makes this plot much more learnable.\"\n",
    "        ),\n",
    "        save_path=os.path.join(OUT_DIR, f\"08_reg_true_vs_pred_{j+1}.png\")\n",
    "    )\n",
    "    # Residuals\n",
    "    res = yt - yp\n",
    "    plt.figure()\n",
    "    plt.hist(res, bins=40)\n",
    "    plt.xlabel(\"Residual (true − predicted)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    show_with_explanation(\n",
    "        title=f\"Regression — {name} — Residual Distribution (Test)\",\n",
    "        explanation=(\n",
    "            \"Purpose: Diagnose error structure.\\n\"\n",
    "            \"How to read: Ideally residuals center around 0 and are roughly symmetric. \"\n",
    "            \"Skew or long tails indicate that some ranges are harder.\\n\"\n",
    "            \"Idea: Residual analysis helps explain limitations: popularity depends on external factors not in features.\"\n",
    "        ),\n",
    "        save_path=os.path.join(OUT_DIR, f\"09_reg_residuals_{j+1}.png\")\n",
    "    )\n",
    "\n",
    "r2_y1 = reg[\"mlp\"][\"r2_y1\"]\n",
    "r2_y2 = reg[\"mlp\"][\"r2_y2\"]\n",
    "r2_y3 = reg[\"mlp\"][\"r2_y3\"]\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.bar([\"y1\", \"y2\", \"y3\"], [r2_y1, r2_y2, r2_y3])\n",
    "ax.set_ylabel(\"R² (Test)\")\n",
    "safe_bar_labels(ax)\n",
    "\n",
    "show_with_explanation(\n",
    "    title=\"Regression — R² per Output (Test)\",\n",
    "    explanation=(\n",
    "        \"Purpose: Summarize how much variance is explained for each regression target.\\n\"\n",
    "        \"How to read: R² = 1 is perfect; 0 means no better than predicting the mean; negative means worse than mean.\\n\"\n",
    "        \"Idea: With social popularity data, modest R² is common; showing improvements from target engineering is key.\"\n",
    "    ),\n",
    "    save_path=os.path.join(OUT_DIR, \"10_reg_R2_per_output.png\")\n",
    ")\n",
    "\n",
    "print(f\"\\nAll figures saved to: {OUT_DIR}/\")\n"
   ],
   "id": "b88ae177c80e0248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:35:31.285617Z",
     "start_time": "2025-12-18T14:35:31.210680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT = Path(\"figures\"); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "def latest(pattern):\n",
    "    files = glob.glob(pattern)\n",
    "    return sorted(files, key=os.path.getmtime)[-1] if files else None\n",
    "\n",
    "paths = {\n",
    "    \"sklearn\": latest(\"results_sklearn_42.json\"),\n",
    "    \"tf\":      latest(\"results_tf_42.json\"),\n",
    "    \"torch\":   latest(\"results_torch_42.json\"),\n",
    "}\n",
    "\n",
    "# Load available results\n",
    "R = {}\n",
    "for k, p in paths.items():\n",
    "    if p is None:\n",
    "        continue\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        R[k] = json.load(f)\n",
    "\n",
    "if not R:\n",
    "    raise FileNotFoundError(\"No results_*.json found. Run training scripts first (torch/tf/sklearn).\")\n",
    "\n",
    "print(\"Loaded files:\")\n",
    "for k, p in paths.items():\n",
    "    if p: print(f\" - {k}: {p}\")\n",
    "print(\"Saving figures to:\", OUT.resolve())\n",
    "\n",
    "def get(d, *keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def save_show(filename, title, xlabel, ylabel, explanation):\n",
    "    plt.title(title)\n",
    "    if xlabel: plt.xlabel(xlabel)\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    fp = OUT / filename\n",
    "    plt.savefig(fp, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(explanation)\n",
    "    print(\"Saved:\", fp)\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "# -------- Collect metrics --------\n",
    "frameworks = list(R.keys())\n",
    "\n",
    "acc = []\n",
    "f1m = []\n",
    "cms = []\n",
    "r2 = {\"y1\": [], \"y2\": [], \"y3\": [], \"mean\": []}\n",
    "rmse = {\"y1\": [], \"y2\": [], \"y3\": []}\n",
    "\n",
    "for fw in frameworks:\n",
    "    cls = get(R[fw], \"results\", \"classification\", default={})\n",
    "    reg = get(R[fw], \"results\", \"regression\", default={})\n",
    "\n",
    "    acc.append(get(cls, \"mlp\", \"accuracy\", default=np.nan))\n",
    "    f1m.append(get(cls, \"mlp\", \"f1_macro\", default=np.nan))\n",
    "    cms.append(get(cls, \"mlp\", \"confusion_matrix\", default=None))\n",
    "\n",
    "    r2[\"y1\"].append(get(reg, \"mlp\", \"r2_y1\", default=np.nan))\n",
    "    r2[\"y2\"].append(get(reg, \"mlp\", \"r2_y2\", default=np.nan))\n",
    "    r2[\"y3\"].append(get(reg, \"mlp\", \"r2_y3\", default=np.nan))\n",
    "    r2[\"mean\"].append(get(reg, \"mlp\", \"r2_mean\", default=np.nan))\n",
    "\n",
    "    rmse[\"y1\"].append(get(reg, \"mlp\", \"rmse_y1\", default=np.nan))\n",
    "    rmse[\"y2\"].append(get(reg, \"mlp\", \"rmse_y2\", default=np.nan))\n",
    "    rmse[\"y3\"].append(get(reg, \"mlp\", \"rmse_y3\", default=np.nan))\n",
    "\n",
    "# -------- 1) Accuracy comparison --------\n",
    "plt.figure()\n",
    "plt.bar(frameworks, acc)\n",
    "plt.ylim(0, 1)\n",
    "save_show(\n",
    "    \"CMP_01_accuracy.png\",\n",
    "    \"Classification Accuracy (Test) — Framework Comparison\",\n",
    "    \"Framework\", \"Accuracy\",\n",
    "    \"Purpose: overall % correct on the test set.\\n\"\n",
    "    \"Idea: accuracy can look good even when the dataset is imbalanced, so we also report Macro-F1.\"\n",
    ")\n",
    "\n",
    "# -------- 2) Macro-F1 comparison --------\n",
    "plt.figure()\n",
    "plt.bar(frameworks, f1m)\n",
    "plt.ylim(0, 1)\n",
    "save_show(\n",
    "    \"CMP_02_macro_f1.png\",\n",
    "    \"Classification Macro-F1 (Test) — Framework Comparison\",\n",
    "    \"Framework\", \"Macro-F1\",\n",
    "    \"Purpose: balanced classification quality across both classes (viral and non-viral).\\n\"\n",
    "    \"Idea: Macro-F1 penalizes models that ignore the minority class; it’s more meaningful than accuracy here.\"\n",
    ")\n",
    "\n",
    "# -------- 3) Confusion matrix per framework --------\n",
    "for fw, cm in zip(frameworks, cms):\n",
    "    if cm is None:\n",
    "        continue\n",
    "    cm = np.array(cm, dtype=int)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n",
    "    plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.colorbar()\n",
    "    save_show(\n",
    "        f\"CMP_03_confusion_{fw}.png\",\n",
    "        f\"Confusion Matrix (Test) — {fw}\",\n",
    "        \"\", \"\",\n",
    "        \"Purpose: shows exactly where the classifier makes mistakes.\\n\"\n",
    "        \"Idea: Top-right = false positives (false viral alerts); bottom-left = false negatives (missed viral).\"\n",
    "    )\n",
    "\n",
    "# -------- 4) Regression R² per output --------\n",
    "x = np.arange(len(frameworks))\n",
    "w = 0.25\n",
    "plt.figure()\n",
    "plt.bar(x - w, r2[\"y1\"], width=w, label=\"R² y1\")\n",
    "plt.bar(x,     r2[\"y2\"], width=w, label=\"R² y2\")\n",
    "plt.bar(x + w, r2[\"y3\"], width=w, label=\"R² y3\")\n",
    "plt.xticks(x, frameworks)\n",
    "plt.legend()\n",
    "save_show(\n",
    "    \"CMP_04_r2_per_output.png\",\n",
    "    \"Regression R² per Output (Test) — Framework Comparison\",\n",
    "    \"Framework\", \"R²\",\n",
    "    \"Purpose: measures how much variance the model explains (higher is better).\\n\"\n",
    "    \"Idea: popularity is noisy, so modest R² is expected; compare frameworks mainly for consistency.\"\n",
    ")\n",
    "\n",
    "# -------- 5) Regression RMSE per output --------\n",
    "plt.figure()\n",
    "plt.bar(x - w, rmse[\"y1\"], width=w, label=\"RMSE y1\")\n",
    "plt.bar(x,     rmse[\"y2\"], width=w, label=\"RMSE y2\")\n",
    "plt.bar(x + w, rmse[\"y3\"], width=w, label=\"RMSE y3\")\n",
    "plt.xticks(x, frameworks)\n",
    "plt.legend()\n",
    "save_show(\n",
    "    \"CMP_05_rmse_per_output.png\",\n",
    "    \"Regression RMSE per Output (Test) — Framework Comparison\",\n",
    "    \"Framework\", \"RMSE\",\n",
    "    \"Purpose: shows error magnitude in the target units.\\n\"\n",
    "    \"Idea: after your y2 fix (log+cap), RMSE across outputs becomes comparable and training is more stable.\"\n",
    ")\n",
    "\n",
    "# -------- 6) Learning curves when present (TF/Torch) --------\n",
    "for fw in frameworks:\n",
    "    cls_hist = get(R[fw], \"results\", \"classification\", \"train_history\", default=None)\n",
    "    if cls_hist and \"train_loss\" in cls_hist and \"val_loss\" in cls_hist:\n",
    "        plt.figure()\n",
    "        plt.plot(cls_hist[\"train_loss\"], label=\"Train loss\")\n",
    "        plt.plot(cls_hist[\"val_loss\"], label=\"Val loss\")\n",
    "        plt.legend()\n",
    "        save_show(\n",
    "            f\"LC_01_class_{fw}.png\",\n",
    "            f\"Classification Learning Curves — {fw}\",\n",
    "            \"Epoch\", \"Loss\",\n",
    "            \"Purpose: verifies learning dynamics and overfitting.\\n\"\n",
    "            \"Idea: if validation loss stops improving, early stopping prevents over-training.\"\n",
    "        )\n",
    "\n",
    "    reg_hist = get(R[fw], \"results\", \"regression\", \"train_history\", default=None)\n",
    "    if reg_hist and \"train_loss\" in reg_hist and \"val_loss\" in reg_hist:\n",
    "        plt.figure()\n",
    "        plt.plot(reg_hist[\"train_loss\"], label=\"Train loss\")\n",
    "        plt.plot(reg_hist[\"val_loss\"], label=\"Val loss\")\n",
    "        plt.legend()\n",
    "        save_show(\n",
    "            f\"LC_02_reg_{fw}.png\",\n",
    "            f\"Regression Learning Curves — {fw}\",\n",
    "            \"Epoch\", \"Loss\",\n",
    "            \"Purpose: checks stability of training for the 3-output regressor.\\n\"\n",
    "            \"Idea: a growing gap (train much lower than val) indicates overfitting.\"\n",
    "        )\n",
    "\n",
    "print(\"Done. All figures saved in:\", OUT.resolve())\n"
   ],
   "id": "e7c3cf96978294a0",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No results_*.json found. Run training scripts first (torch/tf/sklearn).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 27\u001B[0m\n\u001B[0;32m     24\u001B[0m         R[k] \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m R:\n\u001B[1;32m---> 27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo results_*.json found. Run training scripts first (torch/tf/sklearn).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoaded files:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, p \u001B[38;5;129;01min\u001B[39;00m paths\u001B[38;5;241m.\u001B[39mitems():\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: No results_*.json found. Run training scripts first (torch/tf/sklearn)."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fineweb)",
   "language": "python",
   "name": "fineweb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
